{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "- downloading data\n",
    "- loading data\n",
    "- preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2+cpu\n",
      "tiktoken version: 0.5.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from importlib.metadata import version\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    !pip install tiktoken\n",
    "    import tiktoken\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !pip install torch\n",
    "    import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenizers, dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "@dataclass\n",
    "class Params:\n",
    "    DATA_ROOT = r\"./data/\"\n",
    "    BATCH_SIZE=8\n",
    "    MAX_LENGTH=4\n",
    "    STRIDE=1\n",
    "    SHUFFLE=False\n",
    "    VOCAB_SIZE=50257\n",
    "    OUTPUT_DIM=256\n",
    "\n",
    "\n",
    "\n",
    "my_param = Params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(my_param.DATA_ROOT, \"the-verdict.txt\")):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = os.path.join(my_param.DATA_ROOT, \"the-verdict.txt\")\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(my_param.DATA_ROOT, \"the-verdict.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing\n",
    "### tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizing version1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n"
     ]
    }
   ],
   "source": [
    "# split text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# get unique tokens and create vocab dictionary\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding the above text >>> [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "decoding the encoded idx >>> \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# using vocab to indexing\n",
    "tokenizer = tokenizers.SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\n",
    "        \"It's the last he painted, you know,\" \n",
    "        Mrs. Gisburn said with pardonable pride.\n",
    "\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"encoding the above text >>> {ids}\")\n",
    "tks = tokenizer.decode(ids)\n",
    "print(f\"decoding the encoded idx >>> {tks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizing version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# add special tokens \n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined the above two txts >> Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "encoding the txt >>> [1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "then decoding the above index >>> <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizers.SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(f\"combined the above two txts >> {text}\")\n",
    "print(f\"encoding the txt >>> {tokenizer.encode(text)}\")\n",
    "print(f\"then decoding the above index >>> {tokenizer.decode(tokenizer.encode(text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizing version3: GPT2-BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding text >>> [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "decoding the above index >>> Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(f\"encoding text >>> {integers}\")\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(f\"decoding the above index >>> {strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizing version3: GPT2-BPE from scatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding text >>> [15496, 11, 466, 345, 588, 8887, 30, 1279, 91, 437, 1659, 5239, 91, 29, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "decoding the above index >>> Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "orig_tokenizer = tokenizers.get_encoder(model_name=\"gpt2_model\", models_dir=\".\")\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = orig_tokenizer.encode(text)\n",
    "\n",
    "print(f\"encoding text >>> {integers}\")\n",
    "\n",
    "strings = orig_tokenizer.decode(integers)\n",
    "\n",
    "print(f\"decoding the above index >>> {strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch >>> [tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])]\n",
      "second_batch>>> [tensor([[10899,  2138,   257,  7026],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  257,  7026, 15632,   438],\n",
      "        [ 7026, 15632,   438,  2016],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 2016,   257,   922,  5891],\n",
      "        [  257,   922,  5891,  1576]]), tensor([[ 2138,   257,  7026, 15632],\n",
      "        [  257,  7026, 15632,   438],\n",
      "        [ 7026, 15632,   438,  2016],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 2016,   257,   922,  5891],\n",
      "        [  257,   922,  5891,  1576],\n",
      "        [  922,  5891,  1576,   438]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = dataloaders.create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=my_param.BATCH_SIZE, \n",
    "    max_length=my_param.MAX_LENGTH, \n",
    "    stride=my_param.STRIDE, \n",
    "    shuffle=my_param.SHUFFLE\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(f\"first batch >>> {first_batch}\")\n",
    "second_batch = next(data_iter)\n",
    "print(f\"second_batch>>> {second_batch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "- token embedding\n",
    "- positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embedding layer weight >>> Parameter containing:\n",
      "tensor([[-0.3035, -0.5880,  0.3486,  ..., -0.0522, -1.0565,  1.1510],\n",
      "        [-1.3354, -2.9340,  0.1141,  ...,  0.9417, -0.3591,  0.0168],\n",
      "        [-0.1350, -0.5183,  0.2326,  ...,  0.5226,  0.5430,  1.8613],\n",
      "        ...,\n",
      "        [-1.1628,  1.1698,  1.0007,  ...,  0.4479,  0.7890, -0.2578],\n",
      "        [ 1.1263,  1.2176, -1.4959,  ...,  0.3331,  0.3341, -0.2369],\n",
      "        [ 0.7203, -0.1080,  1.0014,  ...,  0.3006,  1.4320,  0.1817]],\n",
      "       requires_grad=True)\n",
      "after token embedding layer >>> tensor([[[ 0.1070, -0.1428, -0.3014,  ..., -2.3238,  0.0778,  0.6690],\n",
      "         [ 0.9873,  0.1164, -0.5726,  ...,  0.3223,  1.2060,  0.2207],\n",
      "         [-1.2633, -0.3237,  0.8158,  ...,  0.1343,  0.8676, -2.5054],\n",
      "         [-0.6525, -0.5808,  1.1188,  ...,  0.1531, -1.1201, -0.5092]],\n",
      "\n",
      "        [[-1.2217,  1.8719,  1.4650,  ..., -1.4138,  0.2478,  1.5313],\n",
      "         [ 1.0885, -0.4509,  0.1388,  ...,  0.7225,  0.1916,  0.1005],\n",
      "         [ 1.2222, -0.0693,  0.7686,  ...,  0.1492,  1.4432,  0.4821],\n",
      "         [-1.2900, -0.4306,  0.3962,  ...,  0.3730, -0.6370,  0.3599]],\n",
      "\n",
      "        [[ 0.1371,  0.0474,  0.5604,  ...,  0.7591,  1.1513,  0.1155],\n",
      "         [ 0.0205, -0.1871, -0.3349,  ..., -1.6650,  1.9221, -0.7920],\n",
      "         [-0.9306,  0.0951, -1.3360,  ..., -1.3698, -1.4675, -0.0698],\n",
      "         [ 0.3131,  0.2846,  0.6302,  ..., -1.1401, -1.4452,  1.3887]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3022,  0.0704, -1.2165,  ...,  0.3273,  0.1405,  1.7339],\n",
      "         [-0.6441, -2.3304,  0.3340,  ..., -1.3910, -1.3095,  1.3439],\n",
      "         [ 2.7396,  0.8004,  0.7119,  ...,  0.9605,  0.4737,  0.7600],\n",
      "         [ 1.0763,  2.1795,  0.9978,  ..., -0.3250, -2.5102,  0.4816]],\n",
      "\n",
      "        [[ 0.9995,  0.8935, -0.0196,  ...,  0.7369,  0.0763,  0.0151],\n",
      "         [-0.0048, -0.4641, -1.6115,  ...,  0.9102, -0.8513, -1.6529],\n",
      "         [ 1.7515, -0.5954,  0.8691,  ...,  2.2617, -0.4027,  0.4138],\n",
      "         [-0.7087,  2.0964, -1.6353,  ..., -0.4251,  0.9527,  1.4568]],\n",
      "\n",
      "        [[ 1.7515, -0.5954,  0.8691,  ...,  2.2617, -0.4027,  0.4138],\n",
      "         [-0.2473, -0.3744,  0.6915,  ..., -0.6520,  0.1543,  0.1991],\n",
      "         [-0.2051,  1.6593, -0.3461,  ..., -0.2971,  0.9366, -0.7977],\n",
      "         [ 0.1582,  0.0067, -0.0738,  ..., -0.3179, -1.1428, -0.1231]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "token embedding shape >>> torch.Size([8, 4, 256])\n",
      "position embedding shape >>> torch.Size([4, 256])\n",
      "position embedding and token embedding shape >>> torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# loading data\n",
    "dataloader = dataloaders.create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=my_param.BATCH_SIZE, \n",
    "    max_length=my_param.MAX_LENGTH,\n",
    "    stride=my_param.MAX_LENGTH, \n",
    "    shuffle=my_param.SHUFFLE\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "# create token embedding layer\n",
    "token_embedding_layer = torch.nn.Embedding(my_param.VOCAB_SIZE, my_param.OUTPUT_DIM)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(f\"token embedding layer weight >>> {token_embedding_layer.weight}\")\n",
    "print(f\"after token embedding layer >>> {token_embeddings}\")\n",
    "print(f\"token embedding shape >>> {token_embeddings.shape}\")\n",
    "\n",
    "# create positional embedding layer\n",
    "context_length = my_param.MAX_LENGTH\n",
    "# GPT-2 uses absolute position embeddings\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, my_param.OUTPUT_DIM)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(my_param.MAX_LENGTH))\n",
    "print(f\"position embedding shape >>> {pos_embeddings.shape}\")\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\"position embedding and token embedding shape >>> {input_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
