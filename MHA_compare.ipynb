{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.2+cpu\n",
      "tiktoken version: 0.5.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from importlib.metadata import version\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    !pip install tiktoken\n",
    "    import tiktoken\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !pip install torch\n",
    "    import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2+cpu\n",
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    context_length = 1024\n",
    "    d_in = 768\n",
    "    d_out = 768\n",
    "    num_heads = 12\n",
    "    batch_size = 8\n",
    "    dropout = 0.0\n",
    "    qkv_bias=False\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    embeddings = torch.randn((batch_size, context_length, d_out), device=device)\n",
    "\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Running on {device}\")\n",
    "\n",
    "my_params = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_params.embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build Multi-Head Attn using several single head attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "time cost >> 3.3361\n"
     ]
    }
   ],
   "source": [
    "mha_wrapper = attentions.MultiHeadAttentionWrapper(\n",
    "    d_in=my_params.d_in, \n",
    "    d_out=my_params.d_out//12,\n",
    "    context_length=my_params.context_length,\n",
    "    dropout=my_params.dropout,\n",
    "    num_heads=my_params.num_heads,\n",
    "    qkv_bias=my_params.qkv_bias\n",
    ").to(my_params.device)\n",
    "\n",
    "t1 = time.time()\n",
    "out = mha_wrapper(my_params.embeddings)\n",
    "t2 = time.time()\n",
    "print(out.shape)\n",
    "print(f\"time cost >> {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build MHA not using wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "time cost >> 3.3689\n"
     ]
    }
   ],
   "source": [
    "mha = attentions.MultiHeadAttention(\n",
    "    d_in=my_params.d_in, \n",
    "    d_out=my_params.d_out,\n",
    "    context_length=my_params.context_length,\n",
    "    dropout=my_params.dropout,\n",
    "    num_heads=my_params.num_heads,\n",
    "    qkv_bias=my_params.qkv_bias\n",
    ").to(my_params.device)\n",
    "\n",
    "t1 = time.time()\n",
    "out = mha(my_params.embeddings)\n",
    "t2 = time.time()\n",
    "print(out.shape)\n",
    "print(f\"time cost >> {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build MHA using single weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "time cost >> 4.8357\n"
     ]
    }
   ],
   "source": [
    "mha_combined_qkv = attentions.MultiHeadAttentionCombinedQKV(\n",
    "    d_in=my_params.d_in, \n",
    "    d_out=my_params.d_out,\n",
    "    context_length=my_params.context_length,\n",
    "    dropout=my_params.dropout,\n",
    "    num_heads=my_params.num_heads,\n",
    "    qkv_bias=my_params.qkv_bias\n",
    ").to(my_params.device)\n",
    "\n",
    "t1 = time.time()\n",
    "out = mha_combined_qkv(my_params.embeddings)\n",
    "t2 = time.time()\n",
    "print(out.shape)\n",
    "print(f\"time cost >> {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build MHA using Pytorch Scaled Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "time cost >> 2.2110\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_scaled = attentions.MHAPyTorchScaledDotProduct(\n",
    "    d_in=my_params.d_in, \n",
    "    d_out=my_params.d_out,\n",
    "    context_length=my_params.context_length,\n",
    "    dropout=my_params.dropout,\n",
    "    num_heads=my_params.num_heads,\n",
    "    qkv_bias=my_params.qkv_bias\n",
    ").to(my_params.device)\n",
    "\n",
    "t1 = time.time()\n",
    "out = mha_pytorch_scaled(my_params.embeddings)\n",
    "t2 = time.time()\n",
    "print(out.shape)\n",
    "print(f\"time cost >> {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build MHA using torch.nn.MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "time cost >> 2.9372\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_class_default = attentions.MHAPyTorchClass(\n",
    "    d_in=my_params.d_in, \n",
    "    d_out=my_params.d_out,\n",
    "    context_length=my_params.context_length,\n",
    "    dropout=my_params.dropout,\n",
    "    num_heads=my_params.num_heads,\n",
    "    qkv_bias=my_params.qkv_bias,\n",
    "    need_weights=True\n",
    ").to(my_params.device)\n",
    "\n",
    "t1 = time.time()\n",
    "out = mha_pytorch_class_default(my_params.embeddings)\n",
    "t2 = time.time()\n",
    "print(out.shape)\n",
    "print(f\"time cost >> {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch's torch.nn.MultiheadAttention with scaled_dot_product_attention\n",
    "- need_weights: If specified, returns attn_output_weights in addition to attn_outputs. Set need_weights=False to use the optimized scaled_dot_product_attention and achieve the best performance for MHA. Default: True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 768])\n",
      "time cost >> 2.8121\n"
     ]
    }
   ],
   "source": [
    "mha_pytorch_class_noweights = attentions.MHAPyTorchClass(\n",
    "    d_in=my_params.d_in, \n",
    "    d_out=my_params.d_out,\n",
    "    context_length=my_params.context_length,\n",
    "    dropout=my_params.dropout,\n",
    "    num_heads=my_params.num_heads,\n",
    "    qkv_bias=my_params.qkv_bias,\n",
    "    need_weights=False # NEW!\n",
    ").to(my_params.device)\n",
    "\n",
    "t1 = time.time()\n",
    "out = mha_pytorch_class_noweights(my_params.embeddings)\n",
    "t2 = time.time()\n",
    "print(out.shape)\n",
    "print(f\"time cost >> {t2-t1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
